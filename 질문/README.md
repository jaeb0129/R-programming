1. 제1/2종오류, p값, 기각역
- 영가설이 참일 때 영가설을 기각: 제1종 오류(type 1 error)
- 영가설이 거짓일 때 영가설을 기각하지 않음: 제2종 오류(type 2 error)
- p값: 영가설을 기각할 확률 
- 기각역: 가설 검정에서 귀무가설을 채택할 수 없는 검정 통계량 값의 영역
  
3. OLS 공식과 의미
- 오차를 최소화 시키는 방법으로 회귀 계수(β0, β1 등)를 추정하는 기법

4. 가능도, MLE
- 표본 데이터를 가지고 likelihood을 활용하여 파라미터 추정하는 방법
- Likelihood: 지금 얻은 데이터가 이 분포로부터 나왔을 가능도
- 수치적으로 이 '가능도'를 계산하는 방법(각 데이터 샘플에서 후보 분포에 대한 높이(즉, likelihood, 기여도)를 계산해서 모두 곱하는 것)

![1](https://github.com/jaeb0129/R-programming/assets/63768509/0d5c067b-887f-42dc-b087-d7172cd450eb)
![2](https://github.com/jaeb0129/R-programming/assets/63768509/dbaa8716-df9e-44a4-952e-273d4af7fe53)

출처: https://blog.naver.com/kisgnues/222677984047

5. 프리퀀티스트 vs 베이지안, 개인적으로 어느 쪽인가

6. 회귀분석의 기본가정, 깨졌을 때 각각 어떻게 대처할까?
- 1) 관측치 제거
> 관측치 제거는 유의해야함

- 2) 변수 변환
> 제곱근 변환, 로그 변환, 제곱 변환, 로짓 변환 등 다양한 변환을 시도
등분산성 가정에 위반된 경우 반응변수의 변환이 도움
선형성 가정이 위반되었을 때 독립변수의 변환

- 3) 변수 추가 또는 제거
- 4) 다른 회귀 방법의 사용

8. CLT의 의미
- " 평균 μ , 표준편차 σ를 가지는 모집단 분포에서 iid 한 표본을 충분히 많이 추출한다면, 표본 평균은 정규분포에 근사하게 된다 ." 라는 정리
- 통상적으로, 우리는 30을 기준으로 표본의 크기가 충분히 큰지, 아닌지 결정

8. 다중공선성이란? 어떻게 탐지 가능할까?
- 다중공선성이란 독립변수들 간에 강한 상관관계가 존재하는 경우를 말한다.
- Variance inflation factor(VIF) 분산팽창요인이 10보다 크다면 다중공선성이 존재한다.
- 그러나 독립(설명)변수들에 몇 개의 선형종속관계가 존재하는지 알 수 없다.

9. GLM을 해 본 적이 있는가? 각 link function별 특징을 얘기해줄 수 있는가?

10. 시계열에서 stationary의 의미
- 정상성(Stationarity)의 정의는 시계열의 확률적인 성질들이 시간의 흐름에 따라 불변한다는 것
- 기댓값과 분산이 시점 t와 무관하게 항상 일정하며 자기공분산이 시점 t가 아닌 시차 k에 의존한다면 이를 약한 의미의 정상성

- 약정상시계열(weak stationary time series)은 다음의 세가지 조건을 만족하는 시계열이다.
![1](https://github.com/jaeb0129/baseball/assets/63768509/58b41381-ae70-4526-9446-c38bf942dc62)
- 임의의 t는 임의의 시점을 의미하는 것입니다. 임의의 t에 대하여 기댓값 E(Xt)가 어떤 값이라는 의미는 어느 시점을 뽑아서 보더라도 기댓값이 같다는 의미입니다.
- 분산 Var(Xt) 역시 이를 바탕으로 이해하시면 됩니다. / 시차(time_lag)에만 의존. 정의의 3번 조건은 Xt와 Xt+h간의 공분산 Cov(Xt+h, Xt)이 γ(h)라고 나타나 있습니다. 이는 어느 시점에 t를 뽑아도, 두 시점 간의 공분산은 그 시간적 차이에만 의존한다고 이해할 수 있습니다.
- 한 문장으로 표현하자면 약 정상성을 띠는 시계열 데이터는 어느 시점(t)에 관측해도 확률 과정의 성질(E(Xt), Var(Xt))이 변하지 않는다
- 어떤 시점에서의 평균과 분산이 일정하고 특정한 시차의 길이를 갖는 자기공분산을 측정하더라도 동일한 값 지닌다면, 정상시계열

### 정상성 검정에는 다양한 검정이 있다.
- ADF(Augmented Dicky-Fuller) 검정
- ADF 검정은 단위근(Unit-root) 검정입니다. 정상성을 판단할 때, ADF 검정은 KPSS와 함께 자주 활용됩니다. ADF 검정의 귀무 가설은 ‘시계열에 단위근이 존재한다’ 이며, 대립가설은 ‘시계열이 정상성을 만족한다’입니다. R에서는 tseries 패키지의 adf.test를 이용

  
11. 회귀분석
- 정규성(normality)
> 종속변수와 잔차 모두 정규분포를 이룹니다. normal Q-Q plot은 표준화된 잔차의 probability plot으로서, 정규성 가정을 만족한다면 이 그래프의 점들은 45도 각도의 직선 위에 있어야 합니다.

- 독립성(independence)
> 독립 변수는 종속 변수의 오류항과 아무런 연관성이 없어서 결과적으로 종속 변수에서 독립돼야 한다. (잔차와 독립변수가 독립-> 독립변수가 변한다고 잔차 변화하면 안됨)
> 종속변수의 오류가 다른 종속 변수의 오류와 상관이 발생하지 않아야 한다.
> 자기 상관 여부 확인 / 더빈 왓슨 검정으로 체크, 1.5 ~ 2.5 사이에 있을 경우 자기 상관이 있다고 판단

- 선형성(linearity)
>종속변수와 독립변수가 선형관계에 있다면 잔차와 예측치 사이에 어떤 체계적인 관계가 있으면 안 됩니다. 잔차와 예측치의 관계를 표현한 그래프(residuals vs. fitted values)에서 무작위 잡음 이외에는 어떤 관계가 보이면 안 됩니다.

- 등분산성(homoscedasticity)
> 설명변수(x)의 값에 관계없이 잔차들의 분산이 일정한 형태를 보임
> 분산이 일정하다면 '(standardized residuals)_sqaured vs. fitted values' 그래프에서 수평선 주위의 random band 형태로 나타나야 합니다.

- 이상치(outlier)
> 회귀모형으로 잘 예측되지 않는 관측치를 의미합니다. 대략 표준 잔차의 2배 이상으로 크거나 -2배 이하로 작은 값은 이상치라고 할 수 있습니다.


- 영향관측치(influential observation)
> 통계 모형 계수를 결정하는 데 불균형한 영향을 미치는 관측치입니다. Cook's distance 사용

- 다중공선성
> 분산팽창지수(variation inflation factor, VIF)라는 통계량을 사용하여 계산할 수 있습니다. 한 예측변수에 대해 VIF의 제곱근은 다중공선성의 정도를 나타내주며, 일반적으로 VIF의 제곱근이 2 이상이라면 다중공선성 문제가 있다고 생각할 수 있습니다.

#### 다중선형 회귀모델에서 변수의 선택
- 다중선형 회귀모델에서는 종속변수를 설명하는 데 도움이 되는 독립변수가 다수 존재
- 그런데 모든 독립변수가 종속변수를 설명하는 데 동일하게 기여하는 것은 아님
- 어떤 변수는 기여도가 높고, 어떤 변수는 기여도가 낮음

- 예를 들어 ' 수면시간', '학습시간'은 '성적'을 예측하는 데 중요한 기여를 할 수 있지만, '점심식사 여부'는 '성적'을 예측하는 데 별로 도움이 되지 않는 변수
- 기여도가 낮거나 거의 없는 변수들은 모델에서 제외하는 것이 좋음(적은 변수를 가지고 현실을 잘 설명할 수 있는 것이 좋은 모델이기 떄문)
- R에서는 모델에 기여하는 변수들을 선별할 수 있는 stepAIC( ) 함수를 제공

​
#### 최적 회귀방정식의 선택

 1) 설명변수 선택 : 가능한 범위 내에서 적은 수의 설명변수 포함

 2) 모형 선택 : 모든 가능한 조합의 회귀분석

  - AIC(Akaike information riterion)나 BIC(Bayesian infrmation criterion) 기준으로 가장 적합한 회귀모형을 선택

  - AIC와 BIC가 가장 작은 값을 갖는 모형을 최적의 모형으로 선택

 3) 단계적 변수 선택(Stepwise Variable Selection)

  ① 전진 선택법(forward selection) : 절편만 있는 상수 모형으로부터 시작해 중요하다고 생각되는 설명변수부터 차례로 모형에 추가

ex) step(forward_step0, scope = ~Population + Illiteracy + Life.Exp + Murder + HS.Grad + Frost + Area, direction ="forward")

 ② 후진 제거법(backward elimination) : 독립변수 후보 모두를 포함한 모형에서 출발해 제곱합의 기준으로 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 더이상 유의하지 않은 변수가 없을 때까지 설명변수를 제거하고 이 때의 모형을 선택 

ex) step(backward_step0, scope = ~Population + Illiteracy + Life.Exp + Murder + HS.Grad + Frost + Area, direction ="backward")

③ 단계별 방법(st epwise method) = 전진선택법 + 후진 선택법

  전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 기인해 기존 변수의 중요도가 약화되면 해당 변수를 제거하는 등 단계별로 추가 또는 제거되는 변수의 여부를 검토해 더 이상 없을 때 중단 

ex) step(backward_step0, scope = ~Population + Illiteracy + Life.Exp + Murder + HS.Grad + Frost + Area, direction ="both")

출처: https://blog.naver.com/aaaaakk02/222626269935
[R] 다중선형회귀분석|작성자 진하루

